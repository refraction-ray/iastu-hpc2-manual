This section reviews some general workflow for daily administration on the cluster.

## some scenarios

### add a new user

* Add user account and password info into ansible user playbook and run it.
* The password can be generated by `openssl rand -base64 32`, see [here](https://www.howtogeek.com/howto/30184/10-ways-to-generate-a-random-password-from-the-command-line/) for more approaches to generate random password.
* `sacctmgr add user <name> account=n3` to make user available to slurm. (already merged into ansible workflow)
* `sudo setquota -u <name> 60G 80G 0 0 /` (already merged into ansible workflow)
* mkdir on /DATA path. (already merged into ansible workflow)
* Probably add `<user> cpu usersoft/` line in `/etc/cgrules.conf`. And `sudo cgrulesengd`. (edit in roles/cgroup/files and run ansible cgroup role instead) **Unincluded**
* Activate mathematica by ansible one-liner. **Unincluded**
* ~~Add backup crontab for user home directory. **Unincluded**~~ taken over by restic as a whole

### add new compute nodes

There is bootstrap script hosted on master in /home/ubuntu/bootstrap. You can open a simple http server in this dir on matser. On newly introduced nodes, just run `bash <(curl -s http://192.168.48.10:8000/bt.sh)`. And it is enough to run ansible workflows from master now.

### after reboot

* It is highly suggested that all ansible playbooks to be executed once reboot (at least network and basic for compute node reboot).
* config cgroup as `sudo cgconfigparser -l /etc/cgconfig.conf && sudo cgrulesengd`.
* start tinc vpn by `sudo tincd -n debug`.
* `sudo ethtool -K enp0s31f6 tso off gso off` on master if you like
* iptables (nat rules) on master is not persistent, see [this issue](https://github.com/ansible/ansible/issues/25149) for further develpment of ansible to incorporate persistence of iptables.
* ~~hostname is not persistent by hostname module of ansible!! [see issue](https://github.com/ansible/ansible/issues/54755)~~ Solved by switch option in cloud.cfg.
* MTU is not persistent by netplan, due to default cloud init in ubuntu (no good even after add mac match to netplan...)
* may need to set `scontrol update nodename=cx state=IDLE` by hand to make them online again in slurm

### summary on works beyond ansible workflow

All extras in master nodes, keep the bottom line that all tasks on compute node should be merged into ansible workflow.

* local nonsytem disk partition and make filesystem
* hard disk mount and fstab configure (one time forever, required before **basic roles**, actually can easily merged into basic role)~~ already merged into ansible workflow
* ~~Possible nvidia drivers install and reboot if GPU is available.~~(already merged into ansible workflow) Cuda and cudnn can be managed by spack.
* quota initial configure (one time forever, required before **user roles)**
* intel parallel studio install (one time forever) (no need to install before any roles, possible issue for python path maybe in **python roles**)
* mathematica install and add virtual mathematica packages in spack (one time forever) (no need to install before any roles). Similar for Matlab (but it has a predefined recipe).
* ~~backup crontabs (one time forever? maybe find some more advanced tools) (no need to configure before any roles)~~ changed to use restic for backup
* python packages install and jupyter configure (continuing work) (no need to install before any roles)
* spack packages install by specs and spack env maintenance (continuing work) (no need to install before any roles)
* sacctmgr ~~cluster~~, qos, priority and account add (continuing work for advanced scenario, minimum setup required before **user roles** after slurm roles)
* ~~two line of commands to final set up ELK stack on master (should find some more elegant way in the future)~~  merged into ansible workflow
* tinc vpn set up on master node
* docker set up on master node
* ~~restic backup setup on master node~~, merged into ansible workflow

### some checks by hand in a lower frequency

There might be some checks running in week basis. These checks should be run manually.

* Smartctl related hard disk healthy check
* restic backup integration and snapshots check
* check `postqueue -p` on each node, to see whether some mails may be stalled.

### install softwares or libraries beyond spack

Installation path: large size commercial softwares: `/opt/softwarename/version/`. Open source library from source: `/home/ubuntu/softwares/softwarename`, in this dir, you may have some name+ver dir for installed versions of softwares and some dir name ended with src as source files. Hopefully, one should put a self-contained information file in each software dir. The name convention is `softwarename.info`. The content includes what each dir within for, what the notes and warnings for this software configuration and installation, most importantly, the install process details for each installed version, such as options for `./configure` and so on. One may even want to capture all stdout for configure and make on each installed version dir with name `configure.out`, `make.out` and so on. To record this stdout more easily, using `script cmake.out` and then run `cmake`, remember ctrl-d or exit to stop the recording. See [more](https://www.geeksforgeeks.org/script-command-in-linux-with-examples/) on script command in linux.

Finally, you may want to include such libraries under the control of spack. This usually involves two steps. If such library already has a position in spack repo, then you only need to add the external path for this package in spack config packages.yaml. If there is some more further fine tuning on module files to load it, you need further hacking spack config modules.yaml. (All these config change should go under ansible workflow). Besides, if the softwares is not registered in spack, then you need first add a package.py in repo as a placeholder. Something as below is enough.

```python
# placeholder for external mma

from spack import *

class Mathematica(Package):

    homepage = "https://www.wolfram.com/mathematica/"

    version('11.0')
```

The insipration of standard workflow on software installation is from [this post](http://www.admin-magazine.com/HPC/Articles/Warewulf-Cluster-Manager-Development-and-Run-Time/(language)/eng-US).

## Known Issues

* MTU cannot be set by netplan yml file, even after we have included the mac:address line in the yml.

  Current Workaround: Include directly command on ip in ansible playbooks.

* Burst of NFS error logs, in master we have: 

  ```bash
  nfsd4_validate_stateid: 270 callbacks suppressed
  NFSD: client 192.168.*.* testing state ID with incorrect client ID
  ```

  And in client, we have: `NFS: nfs4_reclaim_open_state: Lock reclaim failed ` but with lower frequency. Meanwhile, the IO speed for NFS drop to 1/10 of normal speed.

  Current Workaround: cannot find any better solution currently, but rebooting of clients seem to be fine and make all these errors vanish. Possibly related to some Linux kernel NFS bugs.

* Burst of errors: `watchdog: BUG: soft lockup - CPU#20 stuck for 23s! [192.168.*.10-m:27452]` in one of the compute nodes and final crash of the machine though ping is still available then. 

  Current Workaround: Maybe related to the above bug but not sure. Just a hard reboot solves everything.

* Error log of ganglia client claiming that some python module won't work `/usr/sbin/gmond[2358]: [PYTHON] Can't call the metric handler function for [tcpext_tcploss_percentage] in the python module [netstats]`. But this is not true, a reboot can make these error vanishing and every loaded module is workable for gmond.

  Current Workaround: Restarting ganglia-monitor service should be enough. But it may still happen in a regular basis.

* Sometimes, after restart of gmond, it cannot collect all metric, some are missed.

  Current Workaround: No idea why. Just try restarting gmond service, but it may still not work. In sum, gmond status is somewhat fragile and tend to miss some metric. Maybe related to this [so](https://serverfault.daytorrents.com/a/422273/25640), try configuring `send_metadata_interval` to nonzero if one use unicast for gmond. And be patient after restart, gmond may begin collect missing metric several minutes or hours later.

* In some new machines c[4-8], logrotate doesn't work as expected though the conf is the same as previous machines. `/var/lib/logrotate/status` gives new rotate time while the log isn't rotated at all, this should be the reason, still no idea why status file gives wrong rotate time, though (may be related with cron not sync time with timezone due to lack of restart service).

  Current workaround: `sudo logrotate -v -f /etc/logrotate.conf`. verbose and force rotate

  Possible related to cron service, which need to ve restarted to have the same clock with new timezone setted on the machine.

* (*Fully solved*) Assymetry network performance in LAN, master to cn direction can only run in 666Mb (2/3Gbit).

  The problem is now reduced to master only. (Specifically this nic: [I219](https://ark.intel.com/content/www/us/en/ark/products/82185/intel-ethernet-connection-i219-lm.html)) 

  Possible issue: [post](https://forum.manjaro.org/t/solved-only-half-gigabit-eth-with-intel-i219-lm-v-under-kernel-4-14-to-4-19/58886/5)

  Solution: `sudo ethtool -K enp0s31f6 tso off gso off` from [here](https://wiki.hetzner.de/index.php/Low_performance_with_Intel_i218/i219_NIC/en), not very sure of side effects though. (note this command is not persistent when reboot)

  Related kernel [commit](https://github.com/torvalds/linux/commit/b10effb92e272051dd1ec0d7be56bf9ca85ab927)

* (*Fully solved*) Apache2 module in filebeat doesn't support convert time var for pipelines even by explicitly calling it, thus leaving a time mismatch for apache2/error.log. 

  Current workaround: the support is merged into filebeat very recently later than 6.8.0 release. But you can hack it on your own, see [this issue](https://github.com/elastic/beats/issues/3898).

* docker pull might have permission issue in tmux.